Page rank 
n = int(input("Enter number of nodes: "))
print("Enter adjacency matrix:")
matrix = [list(map(int, input().split())) for _ in range(n)]

d = float(input("Enter damping factor (example 0.85): "))
iterations = int(input("Enter number of iterations: "))

pr = [1/n] * n   # initial rank

for it in range(iterations + 1):

   print(f"\nIteration: {it}")
   for i in range(n):
       print(f"Node {i}: {pr[i]:.6f}")

   if it == iterations:       # stop after printing last iteration
       break

   new_pr = [(1 - d) / n] * n  # base value

   # distribute rank
   for j in range(n):
       outdeg = sum(matrix[j])
       if outdeg > 0:
           share = pr[j] / outdeg
           for i in range(n):
               if matrix[j][i] == 1:
                   new_pr[i] += d * share

   pr = new_pr                 # update

max_rank = max(pr)
max_page = pr.index(max_rank)

print("\nFINAL PAGE RANKS:", pr)
print(f"Page with maximum rank = Node {max_page}  with rank = {max_rank:.6f}")



Naive bayes 
dataset = []
n = int(input("Enter number of training tuples: "))

print("\nEnter data in this format => A1 A2 A3 ... Class")
print("Example => red sports domestic yes\n")

for i in range(n):
   row = input(f"Enter tuple {i+1}: ").split()
   dataset.append(row)

# Number of attributes (last value is class)
num_attributes = len(dataset[0]) - 1

# Take test tuple (only attributes)
print("\nEnter test tuple (A1 A2 A3 ...)")
test = input("Enter test tuple: ").split()

print("\n=== TRAINING DATASET ENTERED ===")
for r in dataset:
   print(r)

# Identify all classes automatically
classes = sorted({d[num_attributes] for d in dataset})

# Prior probabilities for each class
priors = {}
for c in classes:
   priors[c] = sum(1 for d in dataset if d[num_attributes] == c) / len(dataset)

print("\n=== PRIOR PROBABILITIES ===")
for c in classes:
   print(f"P({c}) = {priors[c]}")

# conditional probability
def cond_prob(attr_index, attr_value, class_value):
   count_class = sum(1 for d in dataset if d[num_attributes] == class_value)
   count_match = sum(1 for d in dataset if d[attr_index] == attr_value and d[num_attributes] == class_value)
   if count_class == 0: return 0
   return count_match / count_class

# compute posterior
posteriors = {}
print("\n=== CONDITIONAL PROBABILITIES ===")
for c in classes:
   post = priors[c]
   print(f"\nFor class = {c}")
   for i in range(num_attributes):
       cp = cond_prob(i, test[i], c)
       print(f"P({test[i]} | {c}) = {cp}")
       post *= cp
   posteriors[c] = post

print("\n=== RESULT ===")
for c in classes:
   print(f"Posterior {c} = {posteriors[c]}")

predicted = max(posteriors, key=posteriors.get)
print("\nPredicted Class =", predicted.upper())




K-Means
def k_means_1d(data, m1, m2):
    step = 1
    while True:
        print(f"\nStep {step}:")
        print(f"m1 = {m1}, m2 = {m2}")

        # Assign elements to clusters
        k1 = []
        k2 = []
        for x in data:
            if abs(x - m1) <= abs(x - m2):
                k1.append(x)
            else:
                k2.append(x)

        print(f"K1 = {k1}")
        print(f"K2 = {k2}")

        # Calculate new means (rounded)
        new_m1 = round(sum(k1) / len(k1)) if k1 else m1
        new_m2 = round(sum(k2) / len(k2)) if k2 else m2

        print(f"New m1 = {new_m1}, New m2 = {new_m2}")

        # Stop if means do not change
        if new_m1 == m1 and new_m2 == m2:
            print("\nMeans are same. Stopping.")
            break

        # Update means for next step
        m1, m2 = new_m1, new_m2
        step += 1

    return (m1, m2), (k1, k2)

# --- Main Program ---
# Take dataset from user
data_input = input("Enter dataset values separated by spaces: ")
data = list(map(int, data_input.split()))

# Take initial means from user
m1_init = int(input("Enter initial mean m1: "))
m2_init = int(input("Enter initial mean m2: "))

final_means, final_clusters = k_means_1d(data, m1_init, m2_init)

print("\nFinal Means:", final_means)
print("Final Clusters:", final_clusters)




APRIORI
from itertools import combinations
# --- INPUT SECTION ---
transactions = []
n = int(input("Enter number of transactions: "))

for i in range(n):
   items = input(f"Enter items for transaction {i+1} (space separated): ").strip().split(' ')
   transactions.append([item.strip() for item in items])

min_support = float(input("Enter minimum support (as decimal, e.g., 0.5 for 50%): "))
min_confidence = float(input("Enter minimum confidence (as decimal, e.g., 0.7 for 70%): "))

# --- APRIORI IMPLEMENTATION ---
# Function to calculate support of an itemset
def get_support(itemset):
   count = sum(1 for t in transactions if set(itemset).issubset(set(t)))
   return count / len(transactions)

# Get all unique items
items = sorted({item for t in transactions for item in t})
frequent_itemsets = []

print("\nFinding Frequent Itemsets...")
for k in range(1, len(items) + 1):
   candidates = list(combinations(items, k))
   current_frequent = [c for c in candidates if get_support(c) >= min_support]
   if not current_frequent:
       break
   frequent_itemsets.extend(current_frequent)
   print(f"Frequent {k}-itemsets:", current_frequent)

# Generate association rules
print("\nAssociation Rules (Support ≥", min_support, ", Confidence ≥", min_confidence, "):")
for itemset in frequent_itemsets:
   if len(itemset) > 1:
       for i in range(1, len(itemset)):
           for left in combinations(itemset, i):
               right = tuple(sorted(set(itemset) - set(left)))
               support = get_support(itemset)
               confidence = support / get_support(left)
               if confidence >= min_confidence:
                   print(f"{set(left)} -> {set(right)} | Support: {support*100:.2f}, Confidence: {confidence*100:.2f}")






DATA DISCRETIZATION 
import numpy as np
import matplotlib.pyplot as plt

# Take dataset as input
data = list(map(int, input("Enter numbers separated by space: ").split()))
# Take number of bins from user
N = int(input("Enter number of bins: "))
# Step 1: Sort the data
data_sorted = sorted(data)

# Step 2: Divide into N bins
bin_size = len(data_sorted) // N
bins = [data_sorted[i:i+bin_size] for i in range(0, len(data_sorted), bin_size)]

print("Original Bins:", bins)

# Bin by Mean
bins_mean = [[int(np.mean(b)) for _ in b] for b in bins]

# Bin by Median
bins_median = [[int(np.median(b)) for _ in b] for b in bins]

# Bin by Boundary
bins_boundary = []
for b in bins:
   left, right = b[0], b[-1]
   new_bin = [left if abs(x-left) <= abs(x-right) else right for x in b]
   bins_boundary.append(new_bin)

print("Bin by Mean:", bins_mean)
print("Bin by Median:", bins_median)
print("Bin by Boundary:", bins_boundary)

# Flatten results
flatten_original = [x for b in bins for x in b]
flatten_mean = [x for b in bins_mean for x in b]
flatten_median = [x for b in bins_median for x in b]
flatten_boundary = [x for b in bins_boundary for x in b]

# Histograms
plt.figure(figsize=(10, 8))
plt.subplot(2,2,1)
plt.hist(flatten_original, bins=5, color='skyblue', edgecolor='black')
plt.title("Original Data Histogram")
plt.subplot(2,2,2)
plt.hist(flatten_mean, bins=5, color='orange', edgecolor='black')
plt.title("Bin by Mean Histogram")
plt.subplot(2,2,3)
plt.hist(flatten_median, bins=5, color='green', edgecolor='black')
plt.title("Bin by Median Histogram")
plt.subplot(2,2,4)
plt.hist(flatten_boundary, bins=5, color='red', edgecolor='black')
plt.title("Bin by Boundary Histogram")
plt.tight_layout()
plt.show()
